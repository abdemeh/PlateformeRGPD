# Benchmark - Benchmarking des Techniques d'Anonymisation

Ce projet a pour objectif de comparer et de mesurer l'efficacitÃ© de diffÃ©rentes mÃ©thodes d'anonymisation de donnÃ©es (masquage, pseudonymisation, gÃ©nÃ©ralisation, perturbation, agrÃ©gation) en utilisant plusieurs critÃ¨res de performance. Il permet d'effectuer des benchmarks sur des jeux de donnÃ©es synthÃ©tiques gÃ©nÃ©rÃ©s en 10k, 100k et 1M lignes, et de visualiser les rÃ©sultats via des graphiques modernes.

## Table des matiÃ¨res

- [Structure du Projet](#structure-du-projet)
- [GÃ©nÃ©ration des Datasets](#gÃ©nÃ©ration-des-datasets)
- [Installation](#installation)
- [Utilisation](#utilisation)
- [FonctionnalitÃ©s Principales](#fonctionnalitÃ©s-principales)
- [DÃ©veloppement et Extension](#dÃ©veloppement-et-extension)

## Structure du Projet

La structure du dossier du projet est la suivante :

```
Benchmark/
â”œâ”€â”€ main.py                          # Script principal pour lancer les benchmarks et gÃ©nÃ©rer les visualisations.
â”œâ”€â”€ generate.py                      # Script pour gÃ©nÃ©rer des datasets synthÃ©tiques (10k, 100k, 1M).
â”œâ”€â”€ benchmarks/                      # Modules de calcul des critÃ¨res de benchmark.
â”‚   â”œâ”€â”€ execution_time.py
â”‚   â”œâ”€â”€ memory_usage.py
â”‚   â”œâ”€â”€ information_loss.py
â”‚   â”œâ”€â”€ reversibility.py
â”‚   â”œâ”€â”€ re_identification.py
â”‚   â”œâ”€â”€ entropy_diversity.py
â”‚   â”œâ”€â”€ data_precision.py
â”‚   â””â”€â”€ scalability.py
â”œâ”€â”€ methods/                         # MÃ©thodes dâ€™anonymisation.
â”‚   â”œâ”€â”€ masquage.py
â”‚   â”œâ”€â”€ pseudonymisation.py
â”‚   â”œâ”€â”€ generalisation.py
â”‚   â”œâ”€â”€ perturbation.py
â”‚   â””â”€â”€ aggregation.py
â”œâ”€â”€ utils/                           # Fonctions utilitaires.
â”‚   â”œâ”€â”€ dataset_loader.py
â”‚   â”œâ”€â”€ visualization.py
â”‚   â””â”€â”€ json_exporter.py
â”œâ”€â”€ datasets/                        # Datasets originaux (non anonymisÃ©s).
â”‚   â”œâ”€â”€ dataset_example_10000.csv
â”‚   â”œâ”€â”€ dataset_example_100000.csv
â”‚   â””â”€â”€ dataset_example_1000000.csv
â””â”€â”€ anonymized_datasets/             # Datasets anonymisÃ©s (gÃ©nÃ©rÃ©s automatiquement).
```

## Installation

1. **Cloner le dÃ©pÃ´t :**

```bash
git clone https://github.com/abdemeh/PlateformeRGPD.git
cd PlateformeRGPD
cd Benchmark
```

2. **Installer les dÃ©pendances :**

```bash
pip install pandas numpy faker psutil python-Levenshtein scipy matplotlib seaborn plotly altair
```

## GÃ©nÃ©ration des Datasets

Pour gÃ©nÃ©rer les datasets de 10k, 100k et 1M lignes, exÃ©cute simplement :

```bash
python generate.py
```

Les fichiers gÃ©nÃ©rÃ©s seront sauvegardÃ©s automatiquement dans le dossier `datasets/`.

## Utilisation

1. **ExÃ©cution du benchmark principal :**

```bash
python main.py
```

2. **Choisissez un dataset :**  
   Le script affichera la liste des fichiers `.csv` disponibles dans `datasets/`.

3. **Lancement de l'anonymisation + benchmark :**  
   Pour chaque mÃ©thode, un nouveau fichier anonymisÃ© est sauvegardÃ© dans `anonymized_datasets/`.  
   Des mÃ©triques sont calculÃ©es automatiquement, puis affichÃ©es dans des graphiques.

4. **Export JSON :**  
   Un fichier JSON est gÃ©nÃ©rÃ© pour rÃ©sumer tous les rÃ©sultats de benchmark.

## FonctionnalitÃ©s Principales

- ğŸ“Š **Visualisation complÃ¨te des benchmarks**  
  Comparaison par mÃ©trique (temps, mÃ©moire, prÃ©cision, etc.) avec graphiques modernes.

- ğŸ” **MÃ©thodes dâ€™anonymisation testÃ©es** :
  - Masquage
  - Pseudonymisation
  - GÃ©nÃ©ralisation
  - Perturbation
  - AgrÃ©gation

- ğŸ§ª **CritÃ¨res de Benchmark Ã©valuÃ©s** :
  - Temps d'exÃ©cution
  - MÃ©moire utilisÃ©e
  - Perte d'information
  - RÃ©versibilitÃ©
  - Risque de rÃ©-identification
  - Entropie / diversitÃ©
  - PrÃ©cision des donnÃ©es
  - Robustesse Ã  lâ€™Ã©chelle (10k, 100k, 1M)

- ğŸ“‚ **SÃ©paration claire des donnÃ©es** :
  - `datasets/` contient les jeux de donnÃ©es initiaux.
  - `anonymized_datasets/` contient les rÃ©sultats anonymisÃ©s.

- ğŸ“¤ **Export des rÃ©sultats en JSON** pour une analyse ou rÃ©utilisation future.

## DÃ©veloppement et Extension

- Ajouter une nouvelle **mÃ©thode** : crÃ©ez un fichier dans `methods/`, implÃ©mentez la transformation, et ajoutez-la au dictionnaire `methods` dans `main.py`.

- Ajouter un **nouveau critÃ¨re** : ajoutez une fonction dans `benchmarks/` et intÃ©grez-la dans `run_benchmarks()`.

- Personnalisez la **visualisation** dans `utils/visualization.py`.
